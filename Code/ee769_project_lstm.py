# -*- coding: utf-8 -*-
"""EE769_Project_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17oqZSgaq404P00XvKJyztovb9Jl1c6pS

Reference Links: 
1. https://www.analyticsvidhya.com/blog/2021/08/predict-the-next-word-of-your-text-using-long-short-term-memory-lstm/
2. https://medium.com/mlearning-ai/an-illustration-of-next-word-prediction-with-state-of-the-art-network-architectures-like-bert-gpt-c0af02921f17
3. https://pub.towardsai.net/building-a-lstm-from-scratch-in-python-1dedd89de8fe
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')

pip install tensorflow

import re
import pickle
import tensorflow as tf
from nltk.tokenize import word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import LSTM, Dense, Embedding
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.backend import categorical_crossentropy
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import load_model
import numpy as np
import torch
from keras.optimizers import Adam
import torch.nn as nn
from nltk.corpus import stopwords

df = open("/content/Dataset.txt").read()

df

df = df.lower()
new_df = re.sub('[^a-z0-9]+',' ', df)            # Regular expression spliting

new_df

tokens = word_tokenize(new_df)
tokens

#stop_words = set(stopwords.words('english'))

#tokens = [w for w in tokens if not w.lower() in stop_words]

#tokens

text_set = []
train_len = 4
for i in range(train_len,len(tokens)+1):
  seq = tokens[i-train_len:i]
  text_set.append(seq)

text_set

#converting the texts into integer sequence
tokenizer = Tokenizer()
tokenizer.fit_on_texts(text_set)

pickle.dump(tokenizer, open('token.pk1', "wb"))

sequences = tokenizer.texts_to_sequences(text_set)
sequences[:10]

sequences=np.asarray(sequences)

#vocabulary size
vocabulary_size = len(tokenizer.word_counts)+1
vocabulary_size

#trainX
train_inputs=sequences[:,:-1]

train_inputs

#input sequence length 
seq_length=train_inputs.shape[1]
seq_length

#trainY
train_targets=sequences[:,-1]

train_targets

#one hot encoding
train_targets = to_categorical(train_targets, num_classes=vocabulary_size)

train_targets

model = Sequential()
model.add(Embedding(vocabulary_size, 10, input_length = 3))                     # Adding Embedding layer
model.add(LSTM(1000, return_sequences = True))                                  # Adding LSTM Layer
model.add(LSTM(1000))                                                           # Adding LSTM Layer
model.add(Dense(1000, activation = "relu"))                                     # Activation relu with dense layer
model.add(Dense(vocabulary_size, activation = "softmax"))                       # Final layer activation softmax

# Model Building and summary
model.build()
model.summary()

checkpoint = ModelCheckpoint("Dataset.h5", monitor = "loss", verbose = 1, save_best_only = True) 
model.compile(loss = "categorical_crossentropy", optimizer = Adam(learning_rate = 0.01))
model.fit(train_inputs, train_targets, epochs = 5, batch_size = 128, callbacks = [checkpoint])

model = load_model("Dataset.h5")
tokenizer = pickle.load(open("token.pk1", "rb"))


def predict_next_word(text, tokenizer, model):

    #preprocess
    text = text.lower().strip()
    
    #converting the text to word tokens
    input_tokens = word_tokenize(text)
    
    #converting the tokens to integer sequence
    sequences = tokenizer.texts_to_sequences([input_tokens])
    
    #converting to array
    sequences=np.asarray(sequences)

    # Predict
    predict = np.argmax(model.predict(sequences))
    predicted = ""

    for key, value in tokenizer.word_index.items():
      if value == predict:
        predicted = key
        break

    return predicted

predict_next_word("what is score", tokenizer, model)

predict_next_word("However little known", tokenizer, model)
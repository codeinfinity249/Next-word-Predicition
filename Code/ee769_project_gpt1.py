# -*- coding: utf-8 -*-
"""EE769_project_gpt1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xYMVpOsH5nX4DfJWyNf6ZAaogd4xhDES

Reference:
1. https://github.com/kurchi1205/Next-word-Prediction-using-Swiftkey-Data/blob/main/GPT%20Model.ipynb

2. https://towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b
"""

pip install transformers

#importing libraries to be used
import nltk
import re
from tqdm import tqdm
import numpy as np
from nltk import tokenize
import pickle
from nltk.tokenize import word_tokenize

# Code Reference : https://towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b
from transformers import OpenAIGPTTokenizer,OpenAIGPTLMHeadModel,TextDataset,TrainingArguments,Trainer,pipeline,DataCollatorForLanguageModeling

nltk.download('punkt')
nltk.download('stopwords')

path = '/content/drive/MyDrive/EE769/Project/data.txt' #path of dataset
data_set = open(path).read().lower() #opening the file and saving as array
text_corpus = tokenize.sent_tokenize(data_set) #tokenizing the dataset
print(len(text_corpus))

#using regex to clean the data
def white_space(t):  #removes extra white spaces
  return re.sub("\s+", " ", t)
def special_character(t): #removes special character
  return re.sub("[^0-9A-Za-z]", "", t)

for i in range(len(text_corpus)):
  text_corpus[i] = white_space(text_corpus[i])
  text_corpus[i] = special_character(text_corpus[i])

count=[]
for i in tqdm(text_corpus):
    count.append(len(i.split()))
print(np.mean(count))

train_set = text_corpus[:int(0.9*len(text_corpus))] #training dataset
test_set = text_corpus[int(0.9*len(text_corpus)):] #testing dataset

train_set = '.'.join(train_set)
test_set = '.'.join(test_set)

with open("train_set.txt", "w") as fp: #pickling
  fp.write(train_set)
with open("test_set.txt", "w") as fp: #pickling
  fp.write(train_set)

tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt") #using OpenAIGPTTokenizer

print('vocabulary size :', tokenizer.vocab_size) #checking vocabulary size of the model
print('Max Sequence length :', tokenizer.model_max_length) #checking max length of the sentence

data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False) #setting for language modeling, during training it will generate train and test data

train_data_set = TextDataset(tokenizer = tokenizer, file_path = 'train_set.txt', overwrite_cache = True, block_size = 19) #convetring tokens to ids
test_data_set = TextDataset(tokenizer = tokenizer, file_path = 'test_set.txt', overwrite_cache = True, block_size = 19)

model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt') #using pretrained GPT model

training_args = TrainingArguments( output_dir = 'gpt_model', overwrite_output_dir = True, per_device_train_batch_size = 64, per_device_eval_batch_size = 64, learning_rate = 0.0005, num_train_epochs = 3) #setting arguments for training
trainer = Trainer(model = model, args = training_args, data_collator = data_collator, train_dataset = train_data_set, eval_dataset = test_data_set) #Initiallizing trainer class object which will do the training

trainer.train() #training the model for 5 epochs

trainer.save_model() #saving the model

trainer.evaluate(test_data_set) #evaluating the model using the test data set

generator = pipeline('text-generation', tokenizer = 'openai-gpt', model = 'gpt_model')

def predict_next():
    tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")
    model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')
    generator = pipeline('text-generation', tokenizer='openai-gpt', model='gpt_model') 

    text = input('Enter the text: ')
    length= len(tokenizer.encode(text, return_tensors='pt')[0])
        
    max_length = length+1
    
    print('Next Word: ')
    print(generator(text , max_length=max_length)[0]['generated_text'].split(' ')[-1])
    print(generator(text , max_length=max_length , num_beams = 5)[0]['generated_text'].split(' ')[-1])
    print(generator(text , max_length=max_length , do_sample=True,temperature = 0.7)[0]['generated_text'].split(' ')[-1])

predict_next()

predict_next()